{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Now that we have defined a cohort, we can extract features from the database for machine learning.\n",
    "\n",
    "This library provides the capability to efficiently extract a count-based feature representation from BigQuery. Here, we will explore some of those capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "from prediction_utils.extraction_utils.featurizer import BigQueryOMOPFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the extraction\n",
    "config_dict = {\n",
    "    \"gcloud_project\": \"som-nero-phi-nigam-starr\",\n",
    "    \"dataset_project\": \"som-rit-phi-starr-prod\",\n",
    "    \"rs_dataset_project\": \"som-nero-phi-nigam-starr\",\n",
    "    \"dataset\": \"starr_omop_cdm5_deid_1pcent_lite_latest\",\n",
    "    \"rs_dataset\": \"temp_dataset\",\n",
    "    \"data_path\": \"/share/pi/nigam/projects/prediction_utils/scratch/\",\n",
    "    \"features_by_analysis_path\": \"features_by_analysis\",\n",
    "    \"merged_name\": \"merged_features_binary\",\n",
    "    \"cohort_name\": \"vignette_cohort_filtered\",\n",
    "    \"row_id_field\": \"prediction_id\",\n",
    "    \"index_date_field\": \"admit_date\",\n",
    "    \"time_bins\": [-365, -180, -90, -30, 0],\n",
    "    \"include_all_history\": True,\n",
    "    \"overwrite\": True,\n",
    "    \"binary\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above configuration, we can now create a featurizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = BigQueryOMOPFeaturizer(**config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The featurizer objects primary methods are `featurize` and `merge_features`. \n",
    "\n",
    "`featurize` performs a series of set of extractions labeled by `analysis_ids` and time bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the list of analysis_ids that are currently defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['condition_occurrence',\n",
       " 'drug_exposure',\n",
       " 'device_exposure',\n",
       " 'measurement',\n",
       " 'procedure_occurrence',\n",
       " 'note_type',\n",
       " 'observation',\n",
       " 'note_nlp',\n",
       " 'measurement_range',\n",
       " 'gender',\n",
       " 'race',\n",
       " 'ethnicity',\n",
       " 'age_group',\n",
       " 'measurement_bin']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.valid_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these analyses can be binned over time. If the list of `time_bins` is defined, the extraction occurs separately over each time bin. If `include_all_history` is True, then the feature extraction will also occur separately over the full history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the featurizer, selecting only the analyses that we would like.\n",
    "Let's generate features for `gender`, `age_group`, `drug_exposure`, and `condition_occurrence`.\n",
    "Features for five time bins will be generated on the basis of the configuration we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurize(\n",
    "    analysis_ids=['gender', 'age_group', 'drug_exposure', 'condition_occurrence']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the results. The data was stored to `os.path.join(config_dict['data_path'], config_dict['features_by_analysis_path'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'age_group', 'drug_exposure', 'condition_occurrence']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(config_dict['data_path'], config_dict['features_by_analysis_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of gender\n",
      "['static']\n",
      "Contents of drug_exposure\n",
      "['bin_-36500_-1', 'bin_-90_-31', 'bin_-365_-181', 'bin_-30_-1', 'bin_-180_-91']\n",
      "Contents of drug_exposure/bin_-36500_-1\n",
      "['features_42.parquet']\n"
     ]
    }
   ],
   "source": [
    "print('Contents of gender')\n",
    "print(os.listdir(\n",
    "    os.path.join(\n",
    "        config_dict['data_path'], \n",
    "        config_dict['features_by_analysis_path'],\n",
    "        'gender'\n",
    "    )\n",
    "))\n",
    "print('Contents of drug_exposure')\n",
    "print(os.listdir(\n",
    "    os.path.join(\n",
    "        config_dict['data_path'], \n",
    "        config_dict['features_by_analysis_path'],\n",
    "        'drug_exposure'\n",
    "    )\n",
    "))\n",
    "print('Contents of drug_exposure/bin_-36500_-1')\n",
    "print(os.listdir(\n",
    "    os.path.join(\n",
    "        config_dict['data_path'], \n",
    "        config_dict['features_by_analysis_path'],\n",
    "        'drug_exposure',\n",
    "        'bin_-36500_-1'\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `merge_features` method can now be used to merge the individual features into a single feature representation. This also constructs a vocabulary of features that maps each unique feature to a unique numberic identifier from `0..vocab_size-1` where `vocab_size` is the number of unique features.\n",
    "\n",
    "The merge procedures can write the results to either Scipy CSR Sparse or Parquet dataset. For datasets that will fit in memory, CSR is recommended, and Parquet if the data is larger than memory.\n",
    "\n",
    "We will run the merge procedure, generating the CSR sparse result, and not the parquet, since the example data is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.merge_features(\n",
    "    create_sparse=True,\n",
    "    create_parquet=False,\n",
    "    existing_vocab_path=None,\n",
    "    **config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocab', 'features_sparse']\n",
      "Contents of vocab\n",
      "['vocab.parquet']\n",
      "Contents of features_sparse\n",
      "['features.gz', 'features_row_id_map.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.path.join(config_dict['data_path'], config_dict['merged_name'])))\n",
    "print('Contents of vocab')\n",
    "print(os.listdir(os.path.join(config_dict['data_path'], config_dict['merged_name'], 'vocab')))\n",
    "print('Contents of features_sparse')\n",
    "print(os.listdir(os.path.join(config_dict['data_path'], config_dict['merged_name'], 'features_sparse')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the results to explore the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = joblib.load(\n",
    "    os.path.join(config_dict['data_path'], config_dict['merged_name'], 'features_sparse', 'features.gz')\n",
    ")\n",
    "\n",
    "row_id_map = pd.read_parquet(\n",
    "    os.path.join(config_dict['data_path'], config_dict['merged_name'], 'features_sparse', 'features_row_id_map.parquet')\n",
    ")\n",
    "\n",
    "vocab = pd.read_parquet(\n",
    "    os.path.join(config_dict['data_path'], config_dict['merged_name'], 'vocab', 'vocab.parquet')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2754x23159 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 155909 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features_row_id</th>\n",
       "      <th>prediction_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3656122440376071607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1038042919890479280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-735874405745870554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-5741866840589643302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1359111829118906538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features_row_id        prediction_id\n",
       "0                0 -3656122440376071607\n",
       "1                1 -1038042919890479280\n",
       "2                2  -735874405745870554\n",
       "3                3 -5741866840589643302\n",
       "4                4 -1359111829118906538"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_id_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_id</th>\n",
       "      <th>feature_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8532_gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8507_gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>age_group_5_age_group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>age_group_14_age_group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>age_group_17_age_group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_id              feature_id\n",
       "0       0             8532_gender\n",
       "1       1             8507_gender\n",
       "2       2   age_group_5_age_group\n",
       "3       3  age_group_14_age_group\n",
       "4       4  age_group_17_age_group"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
